{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Path: C:\\Users\\User\\Downloads\\head-motion-classification/\n"
     ]
    }
   ],
   "source": [
    "# Add files to sys.path\n",
    "from pathlib import Path\n",
    "import sys,os\n",
    "this_path = None\n",
    "try:    # For .py\n",
    "    this_path = str(os.path.dirname(os.path.abspath(__file__))) #str(Path().absolute())+\"/\" # str(os.path.dirname(__file__))\n",
    "except: # For .ipynb\n",
    "    this_path = str(Path().absolute())+\"/\" #str(Path().absolute())+\"/\" # str(os.path.dirname(__file__))\n",
    "print(\"File Path:\", this_path)\n",
    "sys.path.append(os.path.join(this_path, \"kinemats\"))\n",
    "\n",
    "# Import classes\n",
    "import utils  # Utils for generation of files and paths\n",
    "import quaternion_math\n",
    "\n",
    "from plotter.ts_visualization import *\n",
    "import ts_processing\n",
    "import ts_classification\n",
    "\n",
    "# Import data science libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.rcParams['text.usetex'] = True\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Feature based classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example header ['qw_raw', 'qi_raw', 'qj_raw', 'qk_raw', 'qw_vel', 'qi_vel', 'qj_vel', 'qk_vel', 'qw_acc', 'qi_acc', 'qj_acc', 'qk_acc']\n"
     ]
    }
   ],
   "source": [
    "# CONSTANTS\n",
    "import experiment_config\n",
    "from experiment_config import Datasets, DataRepresentation, Classifiers\n",
    "from ts_classification import EnumDistMetrics\n",
    "\n",
    "# All the files generated from this notebook are in a subfolder with this name\n",
    "NOTEBOOK_SUBFOLDER_NAME = '2_FeatureBasedClassifiers/'\n",
    "\n",
    "# Filenames of created files from this script\n",
    "FILENAME_DATASET_QUATERNION = str(experiment_config.PREFIX_DATASET+str(DataRepresentation.Quaternion))      # generates \"dataset_quaternion\"\n",
    "FILENAME_DATASET_EULER = str(experiment_config.PREFIX_DATASET+str(DataRepresentation.Euler))\n",
    "FILENAME_DATASET_YAW = str(experiment_config.PREFIX_DATASET+str(DataRepresentation.Yaw))\n",
    "\n",
    "#### NOTE: This dictionary is reassigned later in the code whenever the datasets are generated.\n",
    "DICT_DATA = {\n",
    "    DataRepresentation.Quaternion:  None,\n",
    "    DataRepresentation.Euler:       None,\n",
    "    DataRepresentation.Yaw:         None,\n",
    "    DataRepresentation.All:         None,\n",
    "}\n",
    "# Dictionary to convert a datarepresentation into a num - To be stored in the numpy array for results\n",
    "DICT_DATA_TO_NUM = { k:i for i,k in enumerate(DICT_DATA.keys())}\n",
    "\n",
    "### Headers of tabular data for feature-based classifiers\n",
    "SUFFIX_DIFFERENTIAL_TRANSFORMATIONS = [\"raw\",\"vel\",\"acc\"]\n",
    "\n",
    "# Combination of each dimension with each transformation\n",
    "HEADER_QUAT = [ h+\"_\"+s for s in SUFFIX_DIFFERENTIAL_TRANSFORMATIONS for h in [\"qw\",\"qi\",\"qj\",\"qk\"] ]\n",
    "HEADER_EULER = [ h+\"_\"+s for s in SUFFIX_DIFFERENTIAL_TRANSFORMATIONS for h in [\"yaw\",\"pitch\",\"roll\"] ]\n",
    "HEADER_YAW = [ h+\"_\"+s for s in SUFFIX_DIFFERENTIAL_TRANSFORMATIONS for h in [\"yaw\"] ]\n",
    "HEADER_ALL = HEADER_QUAT + HEADER_EULER\n",
    "print(f\"Example header {HEADER_QUAT}\")\n",
    "\n",
    "# Setup of overlapping windows to extract features from time series\n",
    "SLIDING_WINDOW_WIDTH_SECS = 1\n",
    "SLIDING_WINDOW_OVERLAP_SECS = 0 # Not used!!\n",
    "\n",
    "#### Classification methods to apply.\n",
    "DICT_CLASSIFIERS = {\n",
    "    Classifiers.MLP:    MLPClassifier(random_state=1, max_iter=300),\n",
    "    Classifiers.KNN:    KNeighborsClassifier(n_neighbors=experiment_config.KNN_N_NEIGH),\n",
    "    Classifiers.DT:     DecisionTreeClassifier(max_depth=experiment_config.DT_MAX_DEPTH, criterion='entropy', random_state=experiment_config.MC_RANDOM_SEED),\n",
    "    Classifiers.RF:     RandomForestClassifier(n_estimators=experiment_config.RF_N_ESTIMATORS, max_depth=experiment_config.RF_MAX_DEPTH, criterion='entropy', random_state=experiment_config.MC_RANDOM_SEED),\n",
    "    Classifiers.GBM:    GradientBoostingClassifier(n_estimators=experiment_config.GBM_N_ESTIMATORS, max_depth=experiment_config.GBM_MAX_DEPTH, criterion='friedman_mse', random_state=experiment_config.MC_RANDOM_SEED)\n",
    "}\n",
    "\n",
    "## K-Fold partition\n",
    "N_SPLITS_CV = experiment_config.CV_NUM_FOLDS # Number of folds for Cross-validation\n",
    "strat_KFold = StratifiedKFold(n_splits=experiment_config.CV_NUM_FOLDS, random_state=experiment_config.MC_RANDOM_SEED, shuffle=True)\n",
    "\n",
    "# Scoring parameters: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "SCORING_METRICS = [\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# UTILITY FUNCTIONS\n",
    "\n",
    "Generate paths to write output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tsinghua\n"
     ]
    }
   ],
   "source": [
    "STR_DATASET = str(experiment_config.DATASET_MAIN)+\"/\"\n",
    "print(experiment_config.DATASET_MAIN)\n",
    "def gen_path_plot(filename):\n",
    "    # Generates full paths for PLOTS just by specifying a name\n",
    "    return utils.generate_complete_path(filename, \\\n",
    "                                        main_folder=experiment_config.PLOT_FOLDER, \\\n",
    "                                        subfolders=STR_DATASET+NOTEBOOK_SUBFOLDER_NAME, \\\n",
    "                                        file_extension=experiment_config.IMG_FORMAT, save_files=experiment_config.EXPORT_PLOTS)\n",
    "\n",
    "def gen_path_temp(filename, subfolders=\"\", extension=experiment_config.TEMP_FORMAT):\n",
    "    # Generates full paths for TEMP FILES just by specifying a name\n",
    "    return utils.generate_complete_path(filename, \\\n",
    "                                        main_folder=experiment_config.TEMP_FOLDER, \\\n",
    "                                        subfolders=STR_DATASET+subfolders, \\\n",
    "                                        file_extension=extension)\n",
    "\n",
    "def gen_path_results(filename, subfolders=\"\", extension=\"\"):\n",
    "    # Generates full paths for RESULTS FILES (like pandas dataframes)\n",
    "    return utils.generate_complete_path(filename, \\\n",
    "                                        main_folder=experiment_config.RESULTS_FOLDER, \\\n",
    "                                        subfolders=STR_DATASET+NOTEBOOK_SUBFOLDER_NAME+subfolders, \\\n",
    "                                        file_extension=extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASETS: Load and preprocess\n",
    "\n",
    "Datasets are adapted to have the reference right-hand coordinate system used: front=1, left=j, up=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>>>LOADING DATASETS\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t>>>LOADING DATASETS\")\n",
    "dataset = None\n",
    "classes = None\n",
    "\n",
    "# Coordinate reference system. All datasets should be transformed to match this coordinate system.\n",
    "AXIS_INSTANCE=0\n",
    "AXIS_TIME=1\n",
    "AXIS_DIMENSIONS=2\n",
    "\n",
    "# Quaternion representation. All datasets should be transformed to match this quaternion representation.\n",
    "# [qw, qi, qj qk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously processed datasets\n",
    "dataset_quaternion  =   utils.load_binaryfile_npy( gen_path_temp( FILENAME_DATASET_QUATERNION ) )\n",
    "dataset_euler       =   utils.load_binaryfile_npy( gen_path_temp( FILENAME_DATASET_EULER ) )\n",
    "dataset_yaw         =   utils.load_binaryfile_npy( gen_path_temp( FILENAME_DATASET_YAW ) )\n",
    "\n",
    "# General variable to extract dataset stats\n",
    "dataset=dataset_quaternion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_config.DATASET_MAIN == Datasets.Tsinghua:\n",
    "        \n",
    "    # Data for combined time series to cluster\n",
    "    labels_filename = experiment_config.DATASET_LABELS # Cluster index TRUE_LABEL\n",
    "    timestamps_filename = experiment_config.DATASET_TIMESTAMPS # Timestamps\n",
    "    labels = pd.read_csv(labels_filename)\n",
    "    timestamps = np.loadtxt(timestamps_filename)\n",
    "    \n",
    "    # # Classes are the labels of the videos \n",
    "    # classes = labels[\"videoId\"].to_numpy(dtype=np.int32)\n",
    "    # # Classes are the user watching the videos\n",
    "    #classes = labels[\"user\"].to_numpy(dtype=np.int32)\n",
    "\n",
    "    classes = labels[experiment_config.CLASS_COLUMN_NAME].to_numpy(dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamps: <class 'numpy.ndarray'> (3601,)\n",
      "Dataset <class 'numpy.ndarray'> (432, 3601, 4)\n",
      "Classes <class 'numpy.ndarray'> (432,)\n",
      "num_classes=9\n",
      "num_ts=432\n",
      "length_ts=3601\n",
      "num_dims=4\n"
     ]
    }
   ],
   "source": [
    "num_classes = np.unique(classes).size\n",
    "\n",
    "if(dataset.ndim == 2):\n",
    "    dataset = np.expand_dims(dataset, axis=2)\n",
    "\n",
    "num_ts = dataset.shape[AXIS_INSTANCE]\n",
    "length_ts = dataset.shape[AXIS_TIME]\n",
    "num_dims = dataset.shape[AXIS_DIMENSIONS]\n",
    "\n",
    "print(\"Timestamps:\", type(timestamps), timestamps.shape)\n",
    "print(\"Dataset\", type(dataset), dataset.shape)\n",
    "print(\"Classes\", type(classes), classes.shape)\n",
    "print(f\"num_classes={num_classes}\")\n",
    "print(f\"num_ts={num_ts}\")\n",
    "print(f\"length_ts={length_ts}\")\n",
    "print(f\"num_dims={num_dims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Summary`\n",
    "\n",
    "Until this point, the head movements are stored as in these data representations: \n",
    "- Quaternion (`dataset_quaternion`)\n",
    "- Euler (`dataset_euler`)\n",
    "- Yaw (`dataset_yaw`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate velocity and acceleration\n",
    "# according to formulas 14-17 in: doi.org/10.1007/s11045-018-0611-3\n",
    " \n",
    "dataset_quat_vel = quaternion_math.quaternion_mult(dataset_quaternion[:,1:,:],quaternion_math.quaternion_conjugate(dataset_quaternion[:,:-1,:]))\n",
    "dataset_quat_acc = quaternion_math.quaternion_mult(dataset_quat_vel[:,1:,:], quaternion_math.quaternion_conjugate(dataset_quat_vel[:,:-1,:]))\n",
    "\n",
    "# dataset_quat_vel = dataset_quaternion[:,1:,:] - dataset_quaternion[:,:-1,:]\n",
    "# dataset_quat_acc = dataset_quat_vel[:,1:,:] - dataset_quat_vel[:,:-1,:]\n",
    "\n",
    "dataset_euler_vel = dataset_euler[:,1:,:] - dataset_euler[:,:-1,:]\n",
    "dataset_euler_acc = dataset_euler_vel[:,1:,:] - dataset_euler_vel[:,:-1,:]\n",
    "\n",
    "dataset_yaw_vel = dataset_yaw[:,1:,:] - dataset_yaw[:,:-1,:]\n",
    "dataset_yaw_acc = dataset_yaw_vel[:,1:,:] - dataset_yaw_vel[:,:-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamps: (3601,)\n",
      "Quat:  (432, 3601, 4), (432, 3600, 4), (432, 3599, 4)\n",
      "Euler: (432, 3601, 3), (432, 3600, 3), (432, 3599, 3)\n",
      "Yaw:   (432, 3601, 1), (432, 3600, 1), (432, 3599, 1)\n",
      ">> Resizing!!\n",
      "Timestamps: (3599,)\n",
      "Quat:  (432, 3599, 4), (432, 3599, 4), (432, 3599, 4)\n",
      "Euler: (432, 3599, 3), (432, 3599, 3), (432, 3599, 3)\n",
      "Yaw:   (432, 3599, 1), (432, 3599, 1), (432, 3599, 1)\n"
     ]
    }
   ],
   "source": [
    "# Make all the arrays the same length, and reshape the timestamps accordingly\n",
    "print(f\"Timestamps: {timestamps.shape}\")\n",
    "print(f\"Quat:  {dataset_quaternion.shape}, {dataset_quat_vel.shape}, {dataset_quat_acc.shape}\")\n",
    "print(f\"Euler: {dataset_euler.shape}, {dataset_euler_vel.shape}, {dataset_euler_acc.shape}\")\n",
    "print(f\"Yaw:   {dataset_yaw.shape}, {dataset_yaw_vel.shape}, {dataset_yaw_acc.shape}\")\n",
    "\n",
    "print(f\">> Resizing!!\")\n",
    "diff_timestamps = timestamps[2:]\n",
    "\n",
    "dataset_quaternion = dataset_quaternion[:,2:,:]\n",
    "dataset_quat_vel = dataset_quat_vel[:,1:,:]\n",
    "\n",
    "dataset_euler = dataset_euler[:,2:,:]\n",
    "dataset_euler_vel = dataset_euler_vel[:,1:,:]\n",
    "\n",
    "dataset_yaw = dataset_yaw[:,2:,:]\n",
    "dataset_yaw_vel = dataset_yaw_vel[:,1:,:]\n",
    "\n",
    "print(f\"Timestamps: {diff_timestamps.shape}\")\n",
    "print(f\"Quat:  {dataset_quaternion.shape}, {dataset_quat_vel.shape}, {dataset_quat_acc.shape}\")\n",
    "print(f\"Euler: {dataset_euler.shape}, {dataset_euler_vel.shape}, {dataset_euler_acc.shape}\")\n",
    "print(f\"Yaw:   {dataset_yaw.shape}, {dataset_yaw_vel.shape}, {dataset_yaw_acc.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Multiple subexperiments:\n",
    "- Quaternion: Raw, Vel, Acc\n",
    "- Euler: Raw, Vel, Acc\n",
    "- Yaw: Raw, Vel, Acc\n",
    "- All: All Quaternion + All Euler (Yaw is included in Euler angles)\n",
    "\n",
    "Each combination with feature-based classifiers:\n",
    "- KNN\n",
    "- DT\n",
    "- RF\n",
    "- GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>>>LOADING/CREATING TABULAR FEATURES\n",
      "['./results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_quat.csv', './results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_euler.csv', './results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_yaw.csv', './results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_all.csv']\n",
      "Trying 1/2 to load files: ['./results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_quat.csv', './results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_euler.csv', './results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_yaw.csv', './results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_all.csv']\n",
      "File ./results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_quat.csv was successfully loaded\n",
      "File ./results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_euler.csv was successfully loaded\n",
      "File ./results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_yaw.csv was successfully loaded\n",
      "File ./results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_all.csv was successfully loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t>>>LOADING/CREATING TABULAR FEATURES\")\n",
    "# Filename of the file containing demographics and HMD movements data\n",
    "features_quaternion_filename = gen_path_results(\"data_features_quat\", subfolders=\"datasets_feature_based/\", extension=\".csv\")\n",
    "features_euler_filename      = gen_path_results(\"data_features_euler\", subfolders=\"datasets_feature_based/\", extension=\".csv\")\n",
    "features_yaw_filename        = gen_path_results(\"data_features_yaw\", subfolders=\"datasets_feature_based/\", extension=\".csv\")\n",
    "features_all_filename        = gen_path_results(\"data_features_all\", subfolders=\"datasets_feature_based/\", extension=\".csv\")\n",
    "\n",
    "# Placeholders\n",
    "features_quat  = None\n",
    "features_euler = None\n",
    "features_yaw   = None\n",
    "features_all   = None\n",
    "\n",
    "### INPUTS / OUTPUTS\n",
    "\"\"\"EDIT CUSTOM FILENAMES\"\"\"\n",
    "input_files = [features_quaternion_filename, features_euler_filename, features_yaw_filename, features_all_filename]\n",
    "\n",
    "print(input_files)\n",
    "\n",
    "RELOAD_TRIES = experiment_config.RELOAD_TRIES\n",
    "# Try to load files maximum two times\n",
    "for tries in range(RELOAD_TRIES):\n",
    "    try:\n",
    "        ### LOAD FILE\n",
    "        print(f\"Trying {tries+1}/{RELOAD_TRIES} to load files: {input_files}\")\n",
    "        \n",
    "        ### CUSTOM SECTION TO READ FILES\n",
    "        \"\"\"EDIT CUSTOM READ\"\"\"\n",
    "        features_quat = pd.read_csv(input_files[0]) # pd.DataFrame\n",
    "        print(f\"File {input_files[0]} was successfully loaded\")\n",
    "        features_euler = pd.read_csv(input_files[1]) # pd.DataFrame\n",
    "        print(f\"File {input_files[1]} was successfully loaded\")\n",
    "        features_yaw = pd.read_csv(input_files[2]) # pd.DataFrame\n",
    "        print(f\"File {input_files[2]} was successfully loaded\")\n",
    "        features_all = pd.read_csv(input_files[3]) # pd.DataFrame\n",
    "        print(f\"File {input_files[3]} was successfully loaded\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        ### CREATE FILE\n",
    "        print(f\"File not found. Creating again! {e}\")\n",
    "\n",
    "        ### CUSTOM SECTION TO CREATE FILES \n",
    "        \"\"\"EDIT CUSTOM WRITE\"\"\"\n",
    "        \n",
    "        # Combine time series to extract joint features\n",
    "        ARRAYS = [  np.concatenate([dataset_quaternion, dataset_quat_vel, dataset_quat_acc], axis=AXIS_DIMENSIONS), \\\n",
    "                    np.concatenate([dataset_euler, dataset_euler_vel, dataset_euler_acc], axis=AXIS_DIMENSIONS), \\\n",
    "                    np.concatenate([dataset_yaw, dataset_yaw_vel, dataset_yaw_acc], axis=AXIS_DIMENSIONS), \\\n",
    "                    np.concatenate([dataset_quaternion, dataset_quat_vel, dataset_quat_acc,                      \n",
    "                                        dataset_euler, dataset_euler_vel, dataset_euler_acc], axis=AXIS_DIMENSIONS)\n",
    "                 ]\n",
    "\n",
    "        HEADERS = [ HEADER_QUAT, HEADER_EULER, HEADER_YAW, HEADER_ALL]\n",
    "\n",
    "        for idx in range(len(ARRAYS)):\n",
    "            array_to_process = ARRAYS[idx]\n",
    "            header_to_process = HEADERS[idx]\n",
    "            data_feat_array, colnames = ts_processing.extract_summary_statistics_with_non_overlapping_time_window(array_to_process, \\\n",
    "                                                                diff_timestamps, \\\n",
    "                                                                class_labels=classes, \\\n",
    "                                                                time_limit_secs=(timestamps[0],timestamps[-1]), \\\n",
    "                                                                window_width_secs=SLIDING_WINDOW_WIDTH_SECS, \\\n",
    "                                                                dim_names = header_to_process)\n",
    "            dataframe_features = pd.DataFrame(data=data_feat_array, columns=colnames)\n",
    "            for i in [1, 2, -1]:\n",
    "                dataframe_features[colnames[i]] = dataframe_features[colnames[i]].astype(int)\n",
    "            dataframe_features.to_csv(input_files[idx], index=False)\n",
    "\n",
    "        ### ---- CONTROL RETRIES\n",
    "        if tries+1 < RELOAD_TRIES:\n",
    "            continue\n",
    "        else:\n",
    "            raise\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `NOTE:` REDEFINITION OF DATASETS\n",
    "The dictionary is redefined now, since the datasets were properly loaded/created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'quaternion' shape: \t(51840, 64)\n",
      "Dataset 'euler' shape: \t(51840, 49)\n",
      "Dataset 'yaw' shape: \t(51840, 19)\n",
      "Dataset 'all' shape: \t(51840, 109)\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = num_classes # Global variable counting number of different classes in the dataset\n",
    "# REDEFINE DICT WITH CORRESPONDING DATASETS\n",
    "DICT_DATA = {\n",
    "    DataRepresentation.Quaternion:  features_quat.copy(),\n",
    "    DataRepresentation.Euler:       features_euler.copy(),\n",
    "    DataRepresentation.Yaw:         features_yaw.copy(),\n",
    "    DataRepresentation.All:         features_all.copy()\n",
    "}\n",
    "\n",
    "for dr, _data in DICT_DATA.items():\n",
    "    print(f\"Dataset '{dr}' shape: \\t{_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature-based Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contains the dataframe with compiled results\n",
    "classif_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t>>>LOADING/CREATING CLASSIFICATION RESULTS\n",
      "Trying 1/2 to load files: ['./results/Tsinghua/2_FeatureBasedClassifiers/classif_results.csv']\n",
      "File not found. Creating again! [Errno 2] No such file or directory: './results/Tsinghua/2_FeatureBasedClassifiers/classif_results.csv'\n",
      "Iterating 20 times\n",
      " | Iteration 1/20 \t> dataset: quaternion \tclassifier: Classifiers.MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed:  2.1min remaining: 11.8min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed:  2.3min remaining:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:  3.1min remaining:   32.6s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Iteration 2/20 \t> dataset: quaternion \tclassifier: Classifiers.KNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed:    5.0s remaining:   28.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed:    5.6s remaining:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:   10.9s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:   11.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Iteration 3/20 \t> dataset: quaternion \tclassifier: Classifiers.DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed:   15.3s remaining:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed:   15.5s remaining:   15.5s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:   24.7s remaining:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:   25.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Iteration 4/20 \t> dataset: quaternion \tclassifier: Classifiers.RF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed:  1.4min remaining:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed:  1.4min remaining:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:  2.3min remaining:   24.0s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Iteration 5/20 \t> dataset: quaternion \tclassifier: Classifiers.GBM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed: 12.3min remaining: 69.9min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed: 12.3min remaining: 12.3min\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed: 22.7min remaining:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed: 22.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t>> FINISHED (dataset,distMetr)\n",
      " | Iteration 6/20 \t> dataset: euler \tclassifier: Classifiers.MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed:  1.7min remaining:  9.4min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed:  1.9min remaining:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:  2.4min remaining:   25.3s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  2.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Iteration 7/20 \t> dataset: euler \tclassifier: Classifiers.KNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed:    2.1s remaining:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed:    4.5s remaining:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:    9.0s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    9.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Iteration 8/20 \t> dataset: euler \tclassifier: Classifiers.DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed:   12.6s remaining:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed:   12.7s remaining:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:   20.4s remaining:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:   20.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Iteration 9/20 \t> dataset: euler \tclassifier: Classifiers.RF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed:  1.4min remaining:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed:  1.4min remaining:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:  2.2min remaining:   23.0s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Iteration 10/20 \t> dataset: euler \tclassifier: Classifiers.GBM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed: 10.0min remaining: 56.4min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed: 10.0min remaining: 10.0min\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed: 18.3min remaining:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed: 18.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t>> FINISHED (dataset,distMetr)\n",
      " | Iteration 11/20 \t> dataset: yaw \tclassifier: Classifiers.MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed:  1.3min remaining:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed:  1.4min remaining:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:  2.0min remaining:   20.9s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Iteration 12/20 \t> dataset: yaw \tclassifier: Classifiers.KNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed:    1.9s remaining:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed:    4.2s remaining:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:    8.2s remaining:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    8.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Iteration 13/20 \t> dataset: yaw \tclassifier: Classifiers.DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed:    4.6s remaining:   26.4s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed:    4.6s remaining:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:    7.6s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    7.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Iteration 14/20 \t> dataset: yaw \tclassifier: Classifiers.RF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed:   50.4s remaining:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed:   50.5s remaining:   50.5s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:  1.4min remaining:   14.3s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Iteration 15/20 \t> dataset: yaw \tclassifier: Classifiers.GBM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed:  3.7min remaining: 20.9min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed:  3.7min remaining:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:  6.7min remaining:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  6.7min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t>> FINISHED (dataset,distMetr)\n",
      " | Iteration 16/20 \t> dataset: all \tclassifier: Classifiers.MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed:  3.0min remaining: 17.1min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed:  3.8min remaining:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:  4.5min remaining:   48.0s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  4.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Iteration 17/20 \t> dataset: all \tclassifier: Classifiers.KNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed:    1.1s remaining:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed:    6.5s remaining:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:   11.3s remaining:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:   11.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Iteration 18/20 \t> dataset: all \tclassifier: Classifiers.DT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed:   27.5s remaining:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed:   27.6s remaining:   27.6s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:   44.4s remaining:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:   44.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Iteration 19/20 \t> dataset: all \tclassifier: Classifiers.RF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed:  2.1min remaining: 12.2min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed:  2.1min remaining:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:  3.5min remaining:   36.6s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Iteration 20/20 \t> dataset: all \tclassifier: Classifiers.GBM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed: 21.9min remaining: 123.9min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed: 21.9min remaining: 21.9min\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed: 40.1min remaining:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed: 40.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t>> FINISHED (dataset,distMetr)\n",
      "Trying 2/2 to load files: ['./results/Tsinghua/2_FeatureBasedClassifiers/classif_results.csv']\n",
      "File ./results/Tsinghua/2_FeatureBasedClassifiers/classif_results.csv was successfully loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t>>>LOADING/CREATING CLASSIFICATION RESULTS\")\n",
    "# Filename of the file containing demographics and HMD movements data\n",
    "classification_results_filename = gen_path_results(experiment_config.RESULTS_FILENAME, extension=\".csv\")\n",
    "\n",
    "### INPUTS / OUTPUTS\n",
    "\"\"\"EDIT CUSTOM FILENAMES\"\"\"\n",
    "input_files = [classification_results_filename]\n",
    "\n",
    "RELOAD_TRIES = experiment_config.RELOAD_TRIES\n",
    "# Try to load files maximum two times\n",
    "for tries in range(RELOAD_TRIES):\n",
    "    try:\n",
    "        ### LOAD FILE\n",
    "        print(f\"Trying {tries+1}/{RELOAD_TRIES} to load files: {input_files}\")\n",
    "        \n",
    "        ### CUSTOM SECTION TO READ FILES\n",
    "        \"\"\"EDIT CUSTOM READ\"\"\"\n",
    "        classif_results = pd.read_csv(input_files[0]) # pd.DataFrame\n",
    "        print(f\"File {input_files[0]} was successfully loaded\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        ### CREATE FILE\n",
    "        print(f\"File not found. Creating again! {e}\")\n",
    "\n",
    "        ### CUSTOM SECTION TO CREATE FILES \n",
    "        \"\"\"EDIT CUSTOM WRITE\"\"\"\n",
    "        \n",
    "        idx = 1 # Iterator for row index\n",
    "        total_iter = len(DICT_DATA.keys()) * len(DICT_CLASSIFIERS.keys()) # How many total iterations are going to be conducted\n",
    "        print(f\"Iterating {total_iter} times\")\n",
    "        # f = IntProgress(min=0, max=classif_results.shape[0])\n",
    "        # display(f)\n",
    "\n",
    "        ### Iterate over datasets and classifiers\n",
    "\n",
    "        # For each dataset\n",
    "        for iter_datarep, iter_data in DICT_DATA.items():\n",
    "            # Apply each classifier model\n",
    "            for iter_classifier,iter_model in DICT_CLASSIFIERS.items():\n",
    "                \n",
    "                # Print messages\n",
    "                if idx%experiment_config.DISPLAY_ITER_STEP==0: \n",
    "                    print(f\" | Iteration {idx}/{total_iter} \\t> dataset: {str(iter_datarep)} \\tclassifier: {str(iter_classifier)}\")\n",
    "                idx = idx + 1\n",
    "\n",
    "                # Separate training and testing sets\n",
    "                iter_datadata = iter_data.drop(['instanceId','timeId','timestamp'], axis='columns')\n",
    "                X = iter_data.drop([\"class\"],axis=\"columns\")\n",
    "                y = iter_data[\"class\"]\n",
    "\n",
    "                # Standardize (z-norm) if model is KNN\n",
    "                if iter_model is DICT_CLASSIFIERS[Classifiers.KNN]:\n",
    "                    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "                # Cross-validation (CV)\n",
    "                cv_results =  cross_validate(iter_model, X, y, cv=strat_KFold, scoring=SCORING_METRICS, n_jobs=experiment_config.N_JOBS_PARALLEL, verbose=3) # \n",
    "\n",
    "                # Add more information to CV results\n",
    "                cv_results[experiment_config.COLUMNS_LABELS[0]] = [ str(iter_datarep) ] * N_SPLITS_CV # Datarep\n",
    "                cv_results[experiment_config.COLUMNS_LABELS[1]] = [ str(iter_classifier) ] * N_SPLITS_CV # Classifier\n",
    "                cv_results[experiment_config.COLUMNS_LABELS[2]] = np.arange(N_SPLITS_CV) # Fold\n",
    "                # for k,v in cv_results.items():\n",
    "                    # print(f\"{k}: \\t{v}\")\n",
    "\n",
    "                # From dict to pandas DataFrame\n",
    "                iteration_results = pd.DataFrame(cv_results.copy(), columns=sorted(cv_results.keys()))\n",
    "\n",
    "                # Extend dataframe\n",
    "                if classif_results is None:\n",
    "                    classif_results = iteration_results\n",
    "                else:\n",
    "                    classif_results = classif_results.append(iteration_results, ignore_index=True)\n",
    "\n",
    "                # END: Classifiers\n",
    "\n",
    "            # } END: Dataset\n",
    "            print(f\"\\t\\t>> FINISHED (dataset,distMetr)\")\n",
    "        # } END: Loop done\n",
    "\n",
    "        # for i in [0,1,2,3]:\n",
    "            # classif_results[experiment_config.COLUMNS_LABELS[i]] = classif_results[experiment_config.COLUMNS_LABELS[i]].astype(int)\n",
    "\n",
    "        # Save files\n",
    "        classif_results.to_csv(input_files[0], index=False)\n",
    "\n",
    "        ### ---- CONTROL RETRIES\n",
    "        if tries+1 < RELOAD_TRIES:\n",
    "            continue\n",
    "        else:\n",
    "            raise\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> FINISHED WITHOUT ERRORS!!\n"
     ]
    }
   ],
   "source": [
    "print(\">> FINISHED WITHOUT ERRORS!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9fb952a9e6e460b8e2319dbdd5bbe40cc785f6679ade5cf0077f2ef42b5e713"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "f3aec1f4fef7a88c2258d5b84a8b82909f076cff2bcb16988c856ebc42b66954"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
