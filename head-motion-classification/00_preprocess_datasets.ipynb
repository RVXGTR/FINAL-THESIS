{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Tasks with Kinematic Time Series from Head-Mounted Displays\n",
    "\n",
    "## **Load and preprocess datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Path: C:\\Users\\darksoul\\Downloads\\head-motion-classification-AIVR21-/\n"
     ]
    }
   ],
   "source": [
    "# Add files to sys.path\n",
    "from pathlib import Path\n",
    "import sys,os\n",
    "this_path = None\n",
    "try:\n",
    "    this_path = str(os.path.dirname(__file__)) #str(Path().absolute())+\"/\" # str(os.path.dirname(__file__))\n",
    "except:\n",
    "    this_path = str(Path().absolute())+\"/\" #str(Path().absolute())+\"/\" # str(os.path.dirname(__file__))\n",
    "print(\"File Path:\", this_path)\n",
    "sys.path.append(os.path.join(this_path, \"kinemats\"))\n",
    "\n",
    "# Enable debugger in IPython with command set_trace()\n",
    "#from IPython.core.debugger import set_trace\n",
    "\n",
    "# Import classes\n",
    "import utils  # Utils for generation of files and paths\n",
    "\n",
    "from data_loader import dataset_Tsinghua\n",
    "\n",
    "# Import data science libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.rcParams['text.usetex'] = True\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "import experiment_config\n",
    "from experiment_config import Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# UTILITY FUNCTIONS\n",
    "\n",
    "Generate paths to write output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tsinghua/\n"
     ]
    }
   ],
   "source": [
    "STR_DATASET = str(experiment_config.DATASET_MAIN)+\"/\"\n",
    "print(STR_DATASET)\n",
    "def gen_path_plot(filename):\n",
    "    # Generates full paths for PLOTS just by specifying a name\n",
    "    return utils.generate_complete_path(filename, \\\n",
    "                                        main_folder=experiment_config.PLOT_FOLDER, \\\n",
    "                                        subfolders=STR_DATASET+NOTEBOOK_SUBFOLDER_NAME, \\\n",
    "                                        file_extension=experiment_config.IMG_FORMAT, save_files=experiment_config.EXPORT_PLOTS)\n",
    "\n",
    "def gen_path_temp(filename, subfolders=\"\", extension=experiment_config.TEMP_FORMAT):\n",
    "    # Generates full paths for TEMP FILES just by specifying a name\n",
    "    return utils.generate_complete_path(filename, \\\n",
    "                                        main_folder=experiment_config.TEMP_FOLDER, \\\n",
    "                                        subfolders=STR_DATASET+subfolders, \\\n",
    "                                        file_extension=extension)\n",
    "\n",
    "def gen_path_dataset(filename, subfolders=\"\", extension=\"\"):\n",
    "    # Generates full paths for RESULTS FILES (like pandas dataframes)\n",
    "    return utils.generate_complete_path(filename, \\\n",
    "                                        main_folder=experiment_config.DATASET_FOLDER, \\\n",
    "                                        subfolders=STR_DATASET+subfolders, \\\n",
    "                                        file_extension=extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tsinghua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_config.DATASET_MAIN == Datasets.Tsinghua:\n",
    "\n",
    "    # Original compressed dataset\n",
    "    dataset_path = experiment_config.DATASET_TSINGHUA_ZIP\n",
    "    # Path of JSON dictionary used to store the data per user\n",
    "    dict_json_name = gen_path_temp('files_index_per_user', extension=\".json\")\n",
    "\n",
    "    data = dataset_Tsinghua.DatasetHeadMovTsinghua(dataset_path, dict_json_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying 1/2 to load files: ['./dataset/Tsinghua/demographics.csv', './temp/Tsinghua/hmd_movements.pickle']\n",
      "File ./dataset/Tsinghua/demographics.csv was successfully loaded\n",
      "File ./temp/Tsinghua/hmd_movements.pickle was successfully loaded\n"
     ]
    }
   ],
   "source": [
    "if experiment_config.DATASET_MAIN == Datasets.Tsinghua:\n",
    "    # Filename of the file containing demographics and HMD movements data\n",
    "    demographics_data_filename = experiment_config.DATASET_DEMOGRAPHICS\n",
    "    original_data_filename= gen_path_temp(\"hmd_movements\", extension=\".pickle\")\n",
    "\n",
    "\n",
    "    ### INPUTS / OUTPUTS\n",
    "    \"\"\"EDIT CUSTOM FILENAMES\"\"\"\n",
    "    input_files = [demographics_data_filename, original_data_filename]\n",
    "\n",
    "    RELOAD_TRIES = experiment_config.RELOAD_TRIES\n",
    "    # Try to load files maximum two times\n",
    "    for tries in range(RELOAD_TRIES):\n",
    "        try:\n",
    "            ### LOAD FILE\n",
    "            print(f\"Trying {tries+1}/{RELOAD_TRIES} to load files: {input_files}\")\n",
    "            \n",
    "            ### CUSTOM SECTION TO READ FILES\n",
    "            \"\"\"EDIT CUSTOM READ\"\"\"\n",
    "            data.demographics = pd.read_csv(input_files[0]) # data.general is a pd.DataFrame\n",
    "            print(f\"File {input_files[0]} was successfully loaded\")\n",
    "            data.original_data = utils.load_pickle(input_files[1]) # data.movement is a Dictionary\n",
    "            print(f\"File {input_files[1]} was successfully loaded\")\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            ### CREATE FILE\n",
    "            print(f\"File not found. Creating again! {e}\")\n",
    "\n",
    "            ### CUSTOM SECTION TO CREATE FILES \n",
    "            \"\"\"EDIT CUSTOM WRITE\"\"\"\n",
    "            # Create JSON with dictionary of structured data\n",
    "            data.generate_file_index()\n",
    "            # Transform the paths in the compressed file into bytes\n",
    "            data.uncompress_data(#debug_users = 15,                      # Load just this users for test purposes\n",
    "                                 #list_unprocessed_users = skip_users_indices     # Users ID with empty data\n",
    "                                )\n",
    "\n",
    "            # # Delete head-movement data of specific video keys\n",
    "            # data.delete_data_from_videos(videos_to_delete)\n",
    "            # print(\"Removing data from specific video keys... Done!\")\n",
    "\n",
    "            # Save files\n",
    "            data.demographics.to_csv(input_files[0], index=False)\n",
    "            utils.create_pickle(data.original_data, input_files[1])\n",
    "\n",
    "            ### ---- CONTROL RETRIES\n",
    "            if tries+1 < RELOAD_TRIES:\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Synchronization with data interpolation\n",
    "***Generate CSV file with summary of sampling frequency and duration***: The CSV file defines the criteria to resample all time series in common length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying 1/2 to load files: ['./dataset/Tsinghua/summary_timeseries.csv']\n",
      "File ./dataset/Tsinghua/summary_timeseries.csv was successfully loaded\n",
      "   experiment  user  video  startingTime  endTime      N   magQuat  \\\n",
      "0           0     1      0         1.247  164.203  14726  1.000005   \n",
      "1           0     1      1         0.000  201.141  18180  0.999997   \n",
      "2           0     1      2         0.021  293.239  26272  1.000006   \n",
      "3           0     1      3         0.000  172.577  15478  0.999998   \n",
      "4           0     1      4         0.021  205.708  18443  1.000005   \n",
      "\n",
      "   avTsampling  avFsampling  \n",
      "0     0.011066    90.367952  \n",
      "1     0.011064    90.384357  \n",
      "2     0.011161    89.598865  \n",
      "3     0.011150    89.687502  \n",
      "4     0.011153    89.665365  \n"
     ]
    }
   ],
   "source": [
    "# Filename of the file containing demographics and HMD movements data\n",
    "sampling_stats_filename = experiment_config.DATASET_SUMMARY # Original sampling stats\n",
    "\n",
    "### INPUTS / OUTPUTS\n",
    "\"\"\"EDIT CUSTOM FILENAMES\"\"\"\n",
    "input_files = [sampling_stats_filename]\n",
    "\n",
    "# Try to load files maximum two times\n",
    "for tries in range(RELOAD_TRIES):\n",
    "    try:\n",
    "        ### LOAD FILE\n",
    "        print(f\"Trying {tries+1}/{RELOAD_TRIES} to load files: {input_files}\")\n",
    "        \n",
    "        ### CUSTOM SECTION TO READ FILES\n",
    "        \"\"\"EDIT CUSTOM READ\"\"\"\n",
    "        sampling_stats = pd.read_csv(input_files[0]) # data.general is a pd.DataFrame\n",
    "        print(f\"File {input_files[0]} was successfully loaded\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        ### CREATE FILE\n",
    "        print(f\"File not found. Creating again! {e}\")\n",
    "\n",
    "        ### CUSTOM SECTION TO CREATE FILES \n",
    "        \"\"\"EDIT CUSTOM WRITE\"\"\"\n",
    "        # Summary of original sampling frequencies\n",
    "        sampling_stats = data.create_original_sampling_summary()\n",
    "        sampling_stats.to_csv(input_files[0], index=False)\n",
    "\n",
    "        ### ---- CONTROL RETRIES\n",
    "        if tries+1 < RELOAD_TRIES:\n",
    "            continue\n",
    "        else:\n",
    "            raise\n",
    "    break\n",
    "\n",
    "print(sampling_stats.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLERP (Spherical Linear Interpolation)\n",
    "\n",
    "Slerp is shorthand for spherical linear interpolation. It refers to constant-speed motion along a unit-radius great circle arc, given the ends and an interpolation parameter. \"A major appeal is that interpolation is carried out as a rotation about a fixed axis at constant angular velocity\" [REF,pg.18](http://web.cs.iastate.edu/~cs577/handouts/quaternion.pdf)\n",
    "\n",
    "Let $p_{0}$ and $p_{1}$ be the first and last points in the arc, let $t$ be the parameter where $0 \\le t \\le 1$. Compute $\\Omega$ as the angle subtended by the arc so that $cos \\Omega = p_{0} \\cdot p_{1}$\n",
    "\n",
    "$Slerp(p_{0},p_{1};t) = \\frac{sin[(1-t)\\Omega]}{sin(\\Omega)}\\cdot p_{0} + \\frac{sin(t\\Omega)}{sin(\\Omega)}\\cdot p_{1}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_config.DATASET_MAIN == Datasets.Tsinghua:\n",
    "    SAMPLING_FREQUENCY = 30\n",
    "    STARTING_TIME_SECS = 35\n",
    "    ENDING_TIME_SECS = 155\n",
    "\n",
    "    EXPERIMENT_ID = 0 # 0: Experiment_1: No instructions to look at video ROI >> 1: Experiment_2: Instruction to focus on video ROI;;; Check dataset paper for description "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This interpolation is common for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying 1/2 to load files: ['./temp/Tsinghua/hmd_movements_resampled.pickle']\n",
      "File ./temp/Tsinghua/hmd_movements_resampled.pickle was successfully loaded\n"
     ]
    }
   ],
   "source": [
    "# Structure with resampled time-series\n",
    "movement_resampled_data_filename = gen_path_temp(\"hmd_movements_resampled\", extension=\".pickle\")\n",
    "\n",
    "### INPUTS / OUTPUTS\n",
    "\"\"\"EDIT CUSTOM FILENAMES\"\"\"\n",
    "input_files = [movement_resampled_data_filename]\n",
    "\n",
    "# Try to load files maximum two times\n",
    "for tries in range(RELOAD_TRIES):\n",
    "    try:\n",
    "        ### LOAD FILE\n",
    "        print(f\"Trying {tries+1}/{RELOAD_TRIES} to load files: {input_files}\")\n",
    "        \n",
    "        ### CUSTOM SECTION TO READ FILES\n",
    "        \"\"\"EDIT CUSTOM READ\"\"\"\n",
    "        data.processed = utils.load_pickle(input_files[0]) \n",
    "        print(f\"File {input_files[0]} was successfully loaded\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        ### CREATE FILE\n",
    "        print(f\"File not found. Creating again! {e}\")\n",
    "\n",
    "        ### CUSTOM SECTION TO CREATE FILES \n",
    "        \"\"\"EDIT CUSTOM WRITE\"\"\"\n",
    "        \n",
    "        \n",
    "        if experiment_config.DATASET_MAIN == Datasets.Tsinghua:\n",
    "            data.resample_movement(experiment_id = EXPERIMENT_ID, sampling_frequency = SAMPLING_FREQUENCY, starting_time = STARTING_TIME_SECS, end_time = ENDING_TIME_SECS)\n",
    "\n",
    "        # Create pickle file with resampled head-movement data\n",
    "        utils.create_pickle(data.processed, input_files[0])\n",
    "\n",
    "        ### ---- CONTROL RETRIES\n",
    "        if tries+1 < RELOAD_TRIES:\n",
    "            continue\n",
    "        else:\n",
    "            raise\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Tsinghua\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of users 48\n",
      "Total number of videos per user 9\n",
      "Total number of time series 432\n",
      "Head movement per video has size: (3601, 5)\n"
     ]
    }
   ],
   "source": [
    "if experiment_config.DATASET_MAIN == Datasets.Tsinghua: # Validate visually interpolation in some users\n",
    "    # Summary of resampled head movement data\n",
    "    num_users = len(data.processed.keys())\n",
    "    videos_per_user = len(data.processed[1].keys())\n",
    "    total_trajectories = num_users * videos_per_user\n",
    "    video_data_rows, video_data_cols = data.processed[1][0].shape\n",
    "\n",
    "    print(\"Total number of users\",num_users)\n",
    "    print(\"Total number of videos per user\",videos_per_user)\n",
    "    print(\"Total number of time series\", total_trajectories )\n",
    "    print(\"Head movement per video has size:\", (video_data_rows, video_data_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying 1/2 to load files: ['./dataset/Tsinghua/labels.csv', './dataset/Tsinghua/timestamps.csv', './dataset/Tsinghua/dataset.npy']\n",
      "File ./dataset/Tsinghua/labels.csv was successfully loaded\n",
      "File ./dataset/Tsinghua/timestamps.csv was successfully loaded\n",
      "File ./dataset/Tsinghua/dataset.npy was successfully loaded\n"
     ]
    }
   ],
   "source": [
    "if experiment_config.DATASET_MAIN == Datasets.Tsinghua: # Validate visually interpolation in some users\n",
    "    # Data for combined time series to cluster\n",
    "    labels_filename = experiment_config.DATASET_LABELS # Cluster index TRUE_LABEL\n",
    "    timestamps_filename = experiment_config.DATASET_TIMESTAMPS # Timestamps\n",
    "    dataset_filename = experiment_config.DATASET_DATA # Resampled data stats\n",
    "\n",
    "    # Load or create dataframe with statistics of initial dataset (58 users, 5 videos)\n",
    "    labels = None\n",
    "    timestamps = None\n",
    "    dataset = None\n",
    "\n",
    "    ### INPUTS / OUTPUTS\n",
    "    \"\"\"EDIT CUSTOM FILENAMES\"\"\"\n",
    "    input_files = [labels_filename, timestamps_filename, dataset_filename]\n",
    "\n",
    "    # Try to load files maximum two times\n",
    "    for tries in range(RELOAD_TRIES):\n",
    "        try:\n",
    "            ### LOAD FILE\n",
    "            print(f\"Trying {tries+1}/{RELOAD_TRIES} to load files: {input_files}\")\n",
    "            \n",
    "            ### CUSTOM SECTION TO READ FILES\n",
    "            \"\"\"EDIT CUSTOM READ\"\"\"\n",
    "            labels = pd.read_csv(input_files[0])\n",
    "            print(f\"File {input_files[0]} was successfully loaded\")\n",
    "            timestamps = np.loadtxt(input_files[1])\n",
    "            print(f\"File {input_files[1]} was successfully loaded\")\n",
    "            dataset = utils.load_binaryfile_npy(input_files[2])\n",
    "            print(f\"File {input_files[2]} was successfully loaded\")\n",
    "\n",
    "        except Exception as e:\n",
    "            ### CREATE FILE\n",
    "            print(f\"File not found. Creating again! {e}\")\n",
    "\n",
    "            ### CUSTOM SECTION TO CREATE FILES \n",
    "            \"\"\"EDIT CUSTOM WRITE\"\"\"\n",
    "            ## Create DataFrame with labels\n",
    "            labels_cols = [\"id\",\"user\",\"videoId\"]\n",
    "            labels = np.empty((total_trajectories, len(labels_cols)))\n",
    "\n",
    "            # All time series are resampled with the same timestamps, just pick one!\n",
    "            timestamps = data.processed[1][0][:,0]\n",
    "\n",
    "            # Contains all the trajectories in array,\n",
    "            dataset = np.empty((total_trajectories, video_data_rows, video_data_cols - 1))  ## The timestamp is in a different array\n",
    "            \n",
    "            # Time series index, used to map them back the original series with their respective user and index.\n",
    "            ts_idx = 0 \n",
    "            # Put together all the structured time series in one numpy array to do distance calculations\n",
    "            for user in range(1,num_users+1): #[0,1]: ### USERS exist in original data from from 1-48, not 0-47\n",
    "                for video in data.processed[user].keys():\n",
    "                    ## CHECK THAT ALL THE QUATERNIONS IN THE VIDEO HAVE MAGNITUDE 1. [Unit Quaternions]\n",
    "                    magnitudes = [np.linalg.norm(data.processed[user][video][row,1:]) for row in range(data.processed[user][video].shape[0])]\n",
    "                    [print(\"Quaternion norm not equal 1+/-0.01\",val, \"user:\", user, \"video\", video, \"row\", i) for i,val in enumerate(magnitudes) if (val > 1.01 or val < 0.99)]\n",
    "\n",
    "                    # Index of which time series corresponded to which video and which user\n",
    "                    labels[ts_idx] = [ts_idx, user, video]\n",
    "\n",
    "                    # Copy the original structured data in two np array with all the trajectories\n",
    "                    dataset[ts_idx,:,:] = data.processed[user][video][:,1:] ## SKIP FIRST COLUMN\n",
    "\n",
    "                    # Time-series Index, combining the structure per user, per video.\n",
    "                    ts_idx += 1\n",
    "\n",
    "            ## SAVE FILES\n",
    "            # Create dataframe with time index\n",
    "            labels = pd.DataFrame(data=labels, columns=labels_cols)\n",
    "            labels.to_csv(input_files[0], index=False)\n",
    "            print(\"Cluster index created at\", input_files[0])\n",
    "\n",
    "            # Save timestamps\n",
    "            np.savetxt(input_files[1], timestamps, fmt='%f') # Supress scientific notation\n",
    "            print(\"Timestamps created at\",input_files[1])\n",
    "\n",
    "            # Create pickle file with combined time-series for clustering\n",
    "            utils.save_binaryfile_npy(dataset, input_files[2])\n",
    "            print(\"Head movement resampled created at\", input_files[2])\n",
    "\n",
    "\n",
    "            ### ---- CONTROL RETRIES\n",
    "            if tries+1 < RELOAD_TRIES:\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "        \n",
    "        # Finish iteration\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> FINISHED WITHOUT ERRORS!!\n"
     ]
    }
   ],
   "source": [
    "print(\">> FINISHED WITHOUT ERRORS!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
